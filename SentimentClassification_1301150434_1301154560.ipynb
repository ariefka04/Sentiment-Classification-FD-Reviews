{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> KLASIFIKASI SENTIMEN ULASAN PRODUK FEMALE DAILY </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXE-3fVKolz9"
   },
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baca Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data berupa CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-KrPuJ37xkXS"
   },
   "outputs": [],
   "source": [
    "def open_file(file_name):\n",
    "    with open(file_name, encoding=\"utf-8\") as csvfile: \n",
    "        next(csvfile)\n",
    "        rawArticles = csv.reader(csvfile, delimiter=',') \n",
    "        all_reviews = [] \n",
    "        all_sentiments = []\n",
    "        for row in rawArticles:\n",
    "            all_reviews.append((row[3].lower()).split())\n",
    "            all_sentiments.append(row[5].lower())\n",
    "    return all_reviews, all_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qrb-ulyMvEz5"
   },
   "outputs": [],
   "source": [
    "def get_stopword(stopwordsfile):\n",
    "    stopwords=[]\n",
    "    file_stopwords = open(stopwordsfile,'r')\n",
    "    row = file_stopwords.readline()\n",
    "    while row:\n",
    "        word = row.strip()\n",
    "        stopwords.append(word)\n",
    "        row = file_stopwords.readline()\n",
    "    file_stopwords.close()\n",
    "    return stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vM-Yy-SvE_D"
   },
   "outputs": [],
   "source": [
    "def stopword_removal(review,stop_words_indo,stop_words_eng):\n",
    "    feature_vector = []\n",
    "    list_no = ['ga','engga','enggak','gak','nggak','ngga','tdk']\n",
    "    for word in review:\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", word) #menghilangkan karakter selain huruf didalam kata\n",
    "        if (word in stop_words_indo or val is None or word in stop_words_eng):\n",
    "            continue\n",
    "        else:\n",
    "            if word in list_no:\n",
    "                word = 'tidak'\n",
    "            feature_vector.append(word)\n",
    "    for_stemming = ' '.join(feature_vector)\n",
    "    return feature_vector, for_stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Emoji, Punctuation, and Symbol Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQvSh7_8vE2p"
   },
   "outputs": [],
   "source": [
    "def emoji_handling(review):\n",
    "    emoji = []\n",
    "    for word in review:\n",
    "        #Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "        word = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))','POS',word)\n",
    "        \n",
    "        #Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "        word = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)','POS',word)\n",
    "\n",
    "        # Sad -- :-(, : (, :(, ):, )-:\n",
    "        word = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' NEG ', word)\n",
    "\n",
    "        # Cry -- :,(, :'(, :\"(, T_T\n",
    "        word = re.sub(r'(:,\\(|:\\'\\(|:\"\\(|T_T)', ' NEG ', word)\n",
    "\n",
    "        emoji.append(word)\n",
    "    return emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5SWmstCvE5h"
   },
   "outputs": [],
   "source": [
    "def punct_handling(review):\n",
    "    #menghilangkan tanda baca\n",
    "    preprocess_review = []\n",
    "    for word in review:\n",
    "        word = word.strip('\\'\"?!,.():;')\n",
    "\n",
    "        #mengkonversi huruf vocal lebih dari satu dan berurutan\n",
    "        word_character = re.compile(r\"(.)\\1+\", re.DOTALL)\n",
    "        word = word_character.sub(r\"\\1\\1\", word)\n",
    "\n",
    "        #menghilangkan tanda - & '\n",
    "        word = re.sub(r'(-|\\')','',word)\n",
    "\n",
    "        preprocess_review.append(word.lower())\n",
    "    return preprocess_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    return punct_handling(emoji_handling(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PDS_bUCjvE8J"
   },
   "outputs": [],
   "source": [
    "def stem_sentences(review):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Negative Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "et3r9pvQvFCM"
   },
   "outputs": [],
   "source": [
    "def negative_handling(review):\n",
    "    negative_review = []\n",
    "    for i in range(len(review)):\n",
    "        word = review[i]\n",
    "        if review[i-1] != 'tidak':\n",
    "            negative_review.append(word)\n",
    "        else:\n",
    "            word = 'tidak_'+word\n",
    "            negative_review.append(word)\n",
    "    return negative_review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitur Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ucv6ZbTZyUOR"
   },
   "outputs": [],
   "source": [
    "def create_freqwords(reviewHandled):\n",
    "    freqOfWord = {}\n",
    "    for sentence in reviewHandled:\n",
    "        for word in sentence:\n",
    "            if word in freqOfWord:\n",
    "                freqOfWord[word] += 1\n",
    "            else:\n",
    "                freqOfWord[word] = 1\n",
    "    return freqOfWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J0Nn8XJ9yUXC"
   },
   "outputs": [],
   "source": [
    "def get_featureextract(review):\n",
    "    words = set(review)\n",
    "    features = {}\n",
    "    for word in feature_list.keys():\n",
    "        features['contains(%s)' % word] = (word in words) \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasifikasi Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews, all_sentiments = open_file('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_indo = get_stopword('stopwordsindo.txt')\n",
    "stop_words_eng = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocess_reviews = []\n",
    "for review in all_reviews:\n",
    "    feature, review_for_stem = stopword_removal(preprocess_review(review),stop_words_indo,stop_words_eng)\n",
    "    preprocess_reviews.append(stem_sentences(review_for_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 496 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokens = []\n",
    "for review in preprocess_reviews:\n",
    "    tokens.append(nltk.word_tokenize(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_SLQMx4o9D3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "handled_reviews = []\n",
    "reviews = []\n",
    "for i in range(len(tokens)):\n",
    "    neg_handled_rev = negative_handling(tokens[i])\n",
    "    handled_reviews.append(neg_handled_rev)\n",
    "    reviews.append((neg_handled_rev,all_sentiments[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "n_reviews=len(all_reviews)\n",
    "randomize = random.sample(range(n_reviews), n_reviews)\n",
    "idx_train = randomize[:(int(n_reviews*0.8))]\n",
    "idx_test = randomize[(int(n_reviews*0.8)):]\n",
    "reviews_train = [reviews[idx] for idx in idx_train]\n",
    "sentiment_train = [all_sentiments[idx] for idx in idx_train]\n",
    "handled_review_train=[handled_reviews[idx] for idx in idx_train]\n",
    "handled_reviews_test=[handled_reviews[idx] for idx in idx_test]\n",
    "reviews_test = [all_reviews[idx] for idx in idx_test]\n",
    "sentiment_test = [all_sentiments[idx] for idx in idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dengan Ekstraksi Fitur Contains Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feature_list = []\n",
    "feature_list = create_freqwords(handled_review_train)\n",
    "training_set = nltk.classify.util.apply_features(get_featureextract,reviews_train)\n",
    "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction = []\n",
    "validation_test = []\n",
    "for handled_reviews in handled_reviews_test:\n",
    "    classify_result = NBClassifier.classify(get_featureextract(handled_reviews))\n",
    "    prediction.append((handled_reviews,classify_result))\n",
    "    validation_test.append(classify_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_true = 0\n",
    "for k,val in enumerate(validation_test):\n",
    "    if val==sentiment_test[k]: \n",
    "        num_true+=1\n",
    "accuracy = (num_true/len(reviews_test))*100\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dengan TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "handled_reviews = []\n",
    "reviews = []\n",
    "for i in range(len(tokens)):\n",
    "    neg_handled_rev = negative_handling(tokens[i])\n",
    "    handled_reviews.append(neg_handled_rev)\n",
    "    reviews.append((' '.join(neg_handled_rev),all_sentiments[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "labels = np.zeros(500)\n",
    "for i in idx_test:\n",
    "    labels[i]=1\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "totalNB = 0\n",
    "totalMatNB = np.zeros((2,2))\n",
    "data = []\n",
    "sentiments = []\n",
    "for i in range(len(reviews)):\n",
    "    data.append(reviews[i][0])\n",
    "    sentiments.append(reviews[i][1])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[400.   0.]\n",
      " [100.   0.]] 0.8\n"
     ]
    }
   ],
   "source": [
    "for train_idx, test_idx in kf.split(data,sentiments):\n",
    "    X_train = [data[i] for i in train_idx]\n",
    "    X_test = [data[i] for i in test_idx]\n",
    "    y_train,y_test = labels[train_idx], labels[test_idx]\n",
    "    vectorizer = TfidfVectorizer(min_df=0.0, max_df=1.0, sublinear_tf=True, use_idf=True, stop_words='english')\n",
    "    train_tf_idf = vectorizer.fit_transform(X_train)\n",
    "    test_tf_idf = vectorizer.transform(X_test)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(train_tf_idf, y_train)\n",
    "    result = model.predict(test_tf_idf)\n",
    "\n",
    "    totalMatNB = totalMatNB + confusion_matrix(y_test, result)\n",
    "    totalNB = totalNB + sum(y_test==result)\n",
    "print(totalMatNB, totalNB/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "update.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
