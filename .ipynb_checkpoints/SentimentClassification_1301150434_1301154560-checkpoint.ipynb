{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> KLASIFIKASI SENTIMEN ULASAN PRODUK FEMALE DAILY </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXE-3fVKolz9"
   },
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baca Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data berupa CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-KrPuJ37xkXS"
   },
   "outputs": [],
   "source": [
    "def open_file(file_name):\n",
    "    with open(file_name, encoding=\"utf-8\") as csvfile: \n",
    "        next(csvfile)\n",
    "        rawArticles = csv.reader(csvfile, delimiter=',') \n",
    "        all_reviews = [] \n",
    "        all_sentiments = []\n",
    "        for row in rawArticles:\n",
    "            all_reviews.append((row[3].lower()).split())\n",
    "            all_sentiments.append(row[5].lower())\n",
    "    return all_reviews, all_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qrb-ulyMvEz5"
   },
   "outputs": [],
   "source": [
    "def get_stopword(stopwordsfile):\n",
    "    stopwords=[]\n",
    "    file_stopwords = open(stopwordsfile,'r')\n",
    "    row = file_stopwords.readline()\n",
    "    while row:\n",
    "        word = row.strip()\n",
    "        stopwords.append(word)\n",
    "        row = file_stopwords.readline()\n",
    "    file_stopwords.close()\n",
    "    return stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vM-Yy-SvE_D"
   },
   "outputs": [],
   "source": [
    "def stopword_removal(review,stop_words_indo,stop_words_eng):\n",
    "    feature_vector = []\n",
    "    list_no = ['ga','engga','enggak','gak','nggak','ngga','tdk']\n",
    "    for word in review:\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", word) #menghilangkan karakter selain huruf didalam kata\n",
    "        if (word in stop_words_indo or val is None or word in stop_words_eng):\n",
    "            continue\n",
    "        else:\n",
    "            if word in list_no:\n",
    "                word = 'tidak'\n",
    "            feature_vector.append(word)\n",
    "    for_stemming = ' '.join(feature_vector)\n",
    "    return feature_vector, for_stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Emoji, Punctuation, and Symbol Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQvSh7_8vE2p"
   },
   "outputs": [],
   "source": [
    "def emoji_handling(review):\n",
    "    emoji = []\n",
    "    for word in review:\n",
    "        #Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "        word = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))','POS',word)\n",
    "        \n",
    "        #Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "        word = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)','POS',word)\n",
    "\n",
    "        # Sad -- :-(, : (, :(, ):, )-:\n",
    "        word = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' NEG ', word)\n",
    "\n",
    "        # Cry -- :,(, :'(, :\"(, T_T\n",
    "        word = re.sub(r'(:,\\(|:\\'\\(|:\"\\(|T_T)', ' NEG ', word)\n",
    "\n",
    "        emoji.append(word)\n",
    "    return emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5SWmstCvE5h"
   },
   "outputs": [],
   "source": [
    "def punct_handling(review):\n",
    "    #menghilangkan tanda baca\n",
    "    preprocess_review = []\n",
    "    for word in review:\n",
    "        word = word.strip('\\'\"?!,.():;')\n",
    "\n",
    "        #mengkonversi huruf vocal lebih dari satu dan berurutan\n",
    "        word_character = re.compile(r\"(.)\\1+\", re.DOTALL)\n",
    "        word = word_character.sub(r\"\\1\\1\", word)\n",
    "\n",
    "        #menghilangkan tanda - & '\n",
    "        word = re.sub(r'(-|\\')','',word)\n",
    "\n",
    "        preprocess_review.append(word.lower())\n",
    "    return preprocess_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    return punct_handling(emoji_handling(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PDS_bUCjvE8J"
   },
   "outputs": [],
   "source": [
    "def stem_sentences(review):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Negative Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "et3r9pvQvFCM"
   },
   "outputs": [],
   "source": [
    "def negative_handling(review):\n",
    "    negative_review = []\n",
    "    for i in range(len(review)):\n",
    "        word = review[i]\n",
    "        if review[i-1] != 'tidak':\n",
    "            negative_review.append(word)\n",
    "        else:\n",
    "            word = 'tidak_'+word\n",
    "            negative_review.append(word)\n",
    "    return negative_review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitur Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ucv6ZbTZyUOR"
   },
   "outputs": [],
   "source": [
    "def create_freqwords(reviewHandled):\n",
    "    freqOfWord = {}\n",
    "    for sentence in reviewHandled:\n",
    "        for word in sentence:\n",
    "            if word in freqOfWord:\n",
    "                freqOfWord[word] += 1\n",
    "            else:\n",
    "                freqOfWord[word] = 1\n",
    "    return freqOfWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J0Nn8XJ9yUXC"
   },
   "outputs": [],
   "source": [
    "def get_featureextract(review):\n",
    "    words = set(review)\n",
    "    features = {}\n",
    "    for word in feature_list.keys():\n",
    "        features['contains(%s)' % word] = (word in words) \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasifikasi Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews, all_sentiments = open_file('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_indo = get_stopword('stopwordsindo.txt')\n",
    "stop_words_eng = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocess_reviews = []\n",
    "for review in all_reviews:\n",
    "    feature, review_for_stem = stopword_removal(preprocess_review(review),stop_words_indo,stop_words_eng)\n",
    "    preprocess_reviews.append(stem_sentences(review_for_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokens = []\n",
    "for review in preprocess_reviews:\n",
    "    tokens.append(nltk.word_tokenize(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_SLQMx4o9D3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "handled_reviews = []\n",
    "reviews = []\n",
    "for i in range(len(tokens)):\n",
    "    neg_handled_rev = negative_handling(tokens[i])\n",
    "    handled_reviews.append(neg_handled_rev)\n",
    "    reviews.append((neg_handled_rev,all_sentiment[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews=len(all_reviews)\n",
    "randomize = random.sample(range(n_reviews), n_reviews)\n",
    "idx_train = randomize[:(int(n_reviews*0.8))]\n",
    "idx_test = randomize[(int(n_reviews*0.8)):]\n",
    "reviews_train = [reviews[idx] for idx in idx_train]\n",
    "sentiment_train = [all_sentiment[idx] for idx in idx_train]\n",
    "handled_review_train=[handled_reviews[idx] for idx in idx_train]\n",
    "handled_reviews_test=[handled_reviews[idx] for idx in idx_test]\n",
    "reviews_test = [all_reviews[idx] for idx in idx_test]\n",
    "sentiment_test = [all_sentiment[idx] for idx in idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dengan Ekstraksi Fitur Contains Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feature_list = []\n",
    "feature_list = create_freqwords(handled_review_train)\n",
    "training_set = nltk.classify.util.apply_features(get_featureextract,reviews_train)\n",
    "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 726 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction = []\n",
    "validation_test = []\n",
    "for handled_reviews in handled_reviews_test:\n",
    "    classify_result = NBClassifier.classify(get_featureextract(handled_reviews))\n",
    "    prediction.append((handled_reviews,classify_result))\n",
    "    validation_test.append(classify_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_true = 0\n",
    "for k,val in enumerate(validation_test):\n",
    "    if val==sentiment_test[k]: \n",
    "        num_true+=1\n",
    "accuracy = (num_true/len(reviews_test))*100\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dengan TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros(500)\n",
    "for i in idx_test:\n",
    "    labels[i]=1\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "totalNB = 0\n",
    "totalMatNB = np.zeros((2,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-d272176ffb14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msublinear_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mX_train_tf_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mX_test_tf_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "for train_idx, test_idx in kf.split(all_reviews,all_sentiments):\n",
    "    X_train = [all_reviews[i] for i in train_idx]\n",
    "    X_test = [all_reviews[i] for i in test_idx]\n",
    "    y_train,y_test = labels[train_idx], labels[test_idx]\n",
    "    vectorizer = TfidfVectorizer(min_df=0.0, max_df=1.0, sublinear_tf=True, use_idf=True, stop_words='english')\n",
    "    X_train_tf_idf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tf_idf = vectorizer.transform(X_test)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_tf_idf, y_train)\n",
    "    result = model.predict(X_test_tf_idf)\n",
    "\n",
    "    totalMatNB = totalMatNB + confusion_matrix(y_test, result)\n",
    "    totalNB = totalNB + sum(y_test==result)\n",
    "    print(train_idx,test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "update.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
